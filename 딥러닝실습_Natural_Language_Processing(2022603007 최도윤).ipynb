{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5evNZsjt2WF"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)\n",
        "\n",
        "Modified by uramoon@kw.ac.kr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMtBnW6Vt2WJ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "감성 분석 (Sentiment Analysis)은 자연어 처리 (Natural Language Processing, NLP)를 이용하여 텍스트의 감정을 파악하는 기술이다. 이 노트북에서는 영화 리뷰가 긍정적인지 부정적인지 분류할 것이다.\n",
        "\n",
        "\"This movie is not very good.\"을 보면 \"very good\"은 긍정적인 감정이지만 \"not\"이 있기 때문에 부정적인 감정으로 분류되어야 한다. 이러한 것을 어떻게 학습시킬 수 있을까?\n",
        "\n",
        "1. 인공 신경망은 숫자를 입력으로 받아들이는데 텍스트를 어떻게 수치 데이터로 변환할지 생각해봐야 한다.\n",
        "2. 문서의 길이는 문서마다 다른데 크기가 각기 다른 입력 데이터를 인공 신경망에 어떻게 입력해야할지 생각해봐야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMoL_tZ7t2WJ"
      },
      "source": [
        "## Flowchart\n",
        "\n",
        "1. Tokenizer를 이용하여 각 단어를 정수로 변환한다. 예) the: 1, and: 2, a: 3, ...\n",
        "2. 임베딩 (embedding)을 통해 각 정수를 n차원 벡터로 변환한다. (가까운 단어는 가깝게 위치)\n",
        "3. 문서를 순환 신경망 (recurrent neural network, RNN)에 입력하여 0 (부정적)부터 1 (긍적적) 사이의 실수를 출력하게 훈련한다.\n",
        "\n",
        "The flowchart of the algorithm is roughly:\n",
        "\n",
        "<img src=\"https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_natural_language_flowchart.png?raw=1\" alt=\"Flowchart NLP\" style=\"width: 300px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdGt4vEDt2WK"
      },
      "source": [
        "## Recurrent Neural Network (RNN)\n",
        "\n",
        "RNN의 기본 유닛은 Recurrent Unit (RU)으로 LSTM (Long-Short-Term-Memory)과 성능 하락을 최소화하면서 LSTM을 단순화한 GRU (Gated Recurrent Unit)가 많이 사용된다.\n",
        "\n",
        "RU는 과거의 상태를 기억하고 있다가 현재의 입력에 대해 자신의 상태를 바꾸면서 값을 출력한다.\n",
        "\n",
        "새로운 상태는 과거의 상태와 현재의 입력에 따라 결정된다. 예를 들어 최근의 입력에 \"not\"이 있었고, 현재의 입력이 \"good\"이라면 새로운 상태는 \"not good\"에 해당하는 부정적인 감정을 기억할 것이다.\n",
        "\n",
        "![Recurrent unit](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_recurrent_unit.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTHceb8_t2WK"
      },
      "source": [
        "### Unrolled Network\n",
        "\n",
        "RNN에서 RU가 다음 입력에 과거의 정보를 전달하는 과정을 다음과 같이 펼쳐서 도식화할 수 있다.\n",
        "\n",
        "RU의 메모리는 0으로 초기화된다.\n",
        "\n",
        "가장 처음 \"this\"가 입력되면 메모리는 새로운 상태를 저장하고 무언가를 출력하지만 출력값을 사용하지는 않는다. 글을 끝까지 읽었을 때의 출력값만 활용할 것이다.\n",
        "\n",
        "두 번째 단어는 \"is\"인데 바로 전에 읽은 \"this\"와 결합하여 새로운 상태를 저장한다.\n",
        "\n",
        "세 번째 단어는 \"not\"인데 \"not\"을 기억하고 있다가 나중에 \"good\"을 읽었을 때 그것을 부정적인 단어로 바꿔주는 역할을 할 것이다.\n",
        "\n",
        "문서의 마지막 단어를 읽었을 때 0부터 1사이의 값을 출력하는데 이를 활용하여 감성 분석을 수행한다.\n",
        "\n",
        "Note that for the sake of clarity, this figure doesn't show the mapping from text-words to integer-tokens and embedding-vectors, as well as the fully-connected Sigmoid layer on the output.\n",
        "\n",
        "![Unrolled network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_unrolled_flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib5anMcIt2WL"
      },
      "source": [
        "### Exploding & Vanishing Gradients\n",
        "\n",
        "매 스텝 새로운 단어가 들어올 때마다 내부 상태를 변경할 경우 기울기 소실 혹은 폭주 문제가 발생할 수 있습니다.\n",
        "\n",
        "하나의 텍스트가 500개의 단어로 구성된다고 했을 때 내부 상태가 500번 변화하게 되는데 각 변화마다 기울기를 곱할 때 기울기가 1 미만이면 그 값이 0에 가까워지고, 기울기가 1보다 크면 그 값은 매우 커집니다.\n",
        "\n",
        "이러한 기울기 소실 / 폭주 문제를 해결하기 위해 매 입력마다 상태가 변화하지 않는 능력을 지닌 장기기억 메모리를 도입한 것이 LSTM과 GRU입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 준비 단계\n",
        "1. 런타임을 GPU로 설정해주세요.\n",
        "2. imdb.py와 download.py 파일을 Colab 환경에 복사해주세요."
      ],
      "metadata": {
        "id": "Pqee1-G7BMYi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwiNseb1t2WM"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NACTD0ZTt2WM"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsG8d_Kst2WN"
      },
      "source": [
        "We need to import several things from Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1NoU7t1Ht2WO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiZrp2Ppt2WP"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "IMDB에서 50,000개의 영화 리뷰를 다운받습니다. Keras에 정제된 IMDB 데이터셋이 있지만 단어를 정수로 변환하는 작업을 직접 수행하기 위해 정제되지 않은 데이터셋을 사용합니다. 50,000개 중 25,000개는 훈련 데이터, 나머지 25,000개는 테스트 데이터입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RZWc7HVCt2WP"
      },
      "outputs": [],
      "source": [
        "import imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfwqtMAtt2WQ"
      },
      "source": [
        "Automatically download and extract the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ShNhaNQ4t2WQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523ce20a-2ebb-4bfd-f14b-f9b86ad2f553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has apparently already been downloaded and unpacked.\n"
          ]
        }
      ],
      "source": [
        "imdb.maybe_download_and_extract()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다운받은 파일 확인\n",
        "폴더를 새로고침하면 다운로드 받은 data 폴더가 보입니다.\n",
        "\n",
        "1. 훈련 데이터 폴더: data/IMDB/aclImdb/train/\n",
        "2. 테스트 데이터 폴더: data/IMDB/aclImdb/test/"
      ],
      "metadata": {
        "id": "rBXbQObrB5lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터의 pos 폴더에서 10점짜리 긍정적 리뷰 확인\n",
        "#  This movie gets better each time I see it (which is quite often).\n",
        "!cat data/IMDB/aclImdb/train/pos/10001_10.txt"
      ],
      "metadata": {
        "id": "KctXlcN9B4Nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec36bdaf-afa3-4755-a45b-d17bb11f1e76"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터의 neg 폴더에서 1점짜리 부정적 리뷰 확인\n",
        "# this story is too painful to watch.\n",
        "!cat data/IMDB/aclImdb/train/neg/10002_1.txt"
      ],
      "metadata": {
        "id": "H0DdsDcDDDDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b07d89f-c4cd-4c14-d99b-e30c52bb4f7b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI5HZG9ut2WQ"
      },
      "source": [
        "## 훈련 데이터와 테스트 데이터 만들기\n",
        "x 값에는 리뷰 내용이 (텍스트), y 값에는 0 (부정) 혹은 1 (긍정)이 기록됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "anIzfJ5-t2WQ"
      },
      "outputs": [],
      "source": [
        "x_train_text, y_train = imdb.load_data(train=True)\n",
        "x_test_text, y_test = imdb.load_data(train=False)\n",
        "\n",
        "# Convert to numpy arrays.\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "N-fgoTa9t2WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d6d3e5-0a0c-47f4-aa96-7a75380d3dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-set size:  25000\n",
            "Test-set size:   25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Train-set size: \", len(x_train_text))\n",
        "print(\"Test-set size:  \", len(x_test_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: 훈련데이터의 0번째 x값(리뷰)과 y값(정답)을 출력해보세요.\n",
        "print(x_train_text[0])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "xI6MDlNIEVng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371288f0-22eb-440d-8d57-e4e159c1603a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probably the first Portuguese film I have seen in my life, and I enjoyed it. The plot is related of how the young army officers took the power in Portugal in 1974, to finally defeat the fascist government of Caetano and to also finalize the wars in the colonies, i.e. Mozambique, Angola, and Guinea (Bissau)- Cape Vert. Most of the events shown in the film reflect with exactitude the behavior of the army officers and soldiers to conduct the coup, of the oppressed people, who were very happy with this new development and the liberty, the resistance of Caetano's men, and also in a subtle way of most conservative officials, including Spinola, who took over as the new president. The Portuguese revolution can be remembered because of the action of several young officers, but for me the most interesting part of the film was when the young captain expressed that Portugal should develop itself democratically, and this is what the country achieved some years after this coup or revolution. The film also shows that the army officers and soldiers never wanted to kill anyone; even the most serious enemies were respected at the end.\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAD8-TAWt2WS"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "인공신경망은 숫자를 입력받기 때문에 우선 각 단어를 tokenizer를 통해 정수로 변환합니다.<br>Tokenizer는 데이터셋에서 가장 많이 등장하는 n개의 단어를 정수로 변환합니다.<br> 예) the: 1, and: 2, a: 3, ...\n",
        "<br>\n",
        "일단 10000개의 단어만 사용하여 모델을 만들어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UZtN0bXxt2WS"
      },
      "outputs": [],
      "source": [
        "num_words = 10000\n",
        "tokenizer = Tokenizer(num_words=num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEZUMyLIt2WS"
      },
      "source": [
        "훈련 데이터와 테스트 데이터에 들어 있는 모든 단어를 변환할 필요가 있기 때문에 훈련 데이터와 테스트 데이터를 함께 tokenizer에 입력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5qXkL1ONt2WS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd43bb8-4692-4936-b50c-36c50d1653d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.1 s, sys: 61.6 ms, total: 10.2 s\n",
            "Wall time: 10.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "data_text = x_train_text + x_test_text\n",
        "tokenizer.fit_on_texts(data_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvJr9Xfyt2WT"
      },
      "source": [
        "Tokenizer가 찾아낸 많이 등장하는 단어들입니다. 해당 단어들은 정수로 변환되고 나머지 단어들은 제거될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZU5AyhO8t2WT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8158d7af-1c01-4c3c-d6c1-407e12b87449"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'a': 3,\n",
              " 'of': 4,\n",
              " 'to': 5,\n",
              " 'is': 6,\n",
              " 'br': 7,\n",
              " 'in': 8,\n",
              " 'it': 9,\n",
              " 'i': 10,\n",
              " 'this': 11,\n",
              " 'that': 12,\n",
              " 'was': 13,\n",
              " 'as': 14,\n",
              " 'for': 15,\n",
              " 'with': 16,\n",
              " 'movie': 17,\n",
              " 'but': 18,\n",
              " 'film': 19,\n",
              " 'on': 20,\n",
              " 'not': 21,\n",
              " 'you': 22,\n",
              " 'are': 23,\n",
              " 'his': 24,\n",
              " 'have': 25,\n",
              " 'be': 26,\n",
              " 'one': 27,\n",
              " 'he': 28,\n",
              " 'all': 29,\n",
              " 'at': 30,\n",
              " 'by': 31,\n",
              " 'an': 32,\n",
              " 'they': 33,\n",
              " 'so': 34,\n",
              " 'who': 35,\n",
              " 'from': 36,\n",
              " 'like': 37,\n",
              " 'or': 38,\n",
              " 'just': 39,\n",
              " 'her': 40,\n",
              " 'out': 41,\n",
              " 'about': 42,\n",
              " 'if': 43,\n",
              " \"it's\": 44,\n",
              " 'has': 45,\n",
              " 'there': 46,\n",
              " 'some': 47,\n",
              " 'what': 48,\n",
              " 'good': 49,\n",
              " 'when': 50,\n",
              " 'more': 51,\n",
              " 'very': 52,\n",
              " 'up': 53,\n",
              " 'no': 54,\n",
              " 'time': 55,\n",
              " 'my': 56,\n",
              " 'even': 57,\n",
              " 'would': 58,\n",
              " 'she': 59,\n",
              " 'which': 60,\n",
              " 'only': 61,\n",
              " 'really': 62,\n",
              " 'see': 63,\n",
              " 'story': 64,\n",
              " 'their': 65,\n",
              " 'had': 66,\n",
              " 'can': 67,\n",
              " 'me': 68,\n",
              " 'well': 69,\n",
              " 'were': 70,\n",
              " 'than': 71,\n",
              " 'much': 72,\n",
              " 'we': 73,\n",
              " 'bad': 74,\n",
              " 'been': 75,\n",
              " 'get': 76,\n",
              " 'do': 77,\n",
              " 'great': 78,\n",
              " 'other': 79,\n",
              " 'will': 80,\n",
              " 'also': 81,\n",
              " 'into': 82,\n",
              " 'people': 83,\n",
              " 'because': 84,\n",
              " 'how': 85,\n",
              " 'first': 86,\n",
              " 'him': 87,\n",
              " 'most': 88,\n",
              " \"don't\": 89,\n",
              " 'made': 90,\n",
              " 'then': 91,\n",
              " 'its': 92,\n",
              " 'them': 93,\n",
              " 'make': 94,\n",
              " 'way': 95,\n",
              " 'too': 96,\n",
              " 'movies': 97,\n",
              " 'could': 98,\n",
              " 'any': 99,\n",
              " 'after': 100,\n",
              " 'think': 101,\n",
              " 'characters': 102,\n",
              " 'watch': 103,\n",
              " 'films': 104,\n",
              " 'two': 105,\n",
              " 'many': 106,\n",
              " 'seen': 107,\n",
              " 'character': 108,\n",
              " 'being': 109,\n",
              " 'never': 110,\n",
              " 'plot': 111,\n",
              " 'love': 112,\n",
              " 'acting': 113,\n",
              " 'life': 114,\n",
              " 'did': 115,\n",
              " 'best': 116,\n",
              " 'where': 117,\n",
              " 'know': 118,\n",
              " 'show': 119,\n",
              " 'little': 120,\n",
              " 'over': 121,\n",
              " 'off': 122,\n",
              " 'ever': 123,\n",
              " 'does': 124,\n",
              " 'your': 125,\n",
              " 'better': 126,\n",
              " 'end': 127,\n",
              " 'man': 128,\n",
              " 'scene': 129,\n",
              " 'still': 130,\n",
              " 'say': 131,\n",
              " 'these': 132,\n",
              " 'here': 133,\n",
              " 'why': 134,\n",
              " 'scenes': 135,\n",
              " 'while': 136,\n",
              " 'something': 137,\n",
              " 'such': 138,\n",
              " 'go': 139,\n",
              " 'through': 140,\n",
              " 'back': 141,\n",
              " 'should': 142,\n",
              " 'those': 143,\n",
              " 'real': 144,\n",
              " \"i'm\": 145,\n",
              " 'now': 146,\n",
              " 'watching': 147,\n",
              " 'thing': 148,\n",
              " \"doesn't\": 149,\n",
              " 'actors': 150,\n",
              " 'though': 151,\n",
              " 'funny': 152,\n",
              " 'years': 153,\n",
              " \"didn't\": 154,\n",
              " 'old': 155,\n",
              " 'another': 156,\n",
              " '10': 157,\n",
              " 'work': 158,\n",
              " 'before': 159,\n",
              " 'actually': 160,\n",
              " 'nothing': 161,\n",
              " 'makes': 162,\n",
              " 'look': 163,\n",
              " 'director': 164,\n",
              " 'find': 165,\n",
              " 'going': 166,\n",
              " 'same': 167,\n",
              " 'new': 168,\n",
              " 'lot': 169,\n",
              " 'every': 170,\n",
              " 'few': 171,\n",
              " 'again': 172,\n",
              " 'part': 173,\n",
              " 'cast': 174,\n",
              " 'down': 175,\n",
              " 'us': 176,\n",
              " 'things': 177,\n",
              " 'want': 178,\n",
              " 'quite': 179,\n",
              " 'pretty': 180,\n",
              " 'world': 181,\n",
              " 'horror': 182,\n",
              " 'around': 183,\n",
              " 'seems': 184,\n",
              " \"can't\": 185,\n",
              " 'young': 186,\n",
              " 'take': 187,\n",
              " 'however': 188,\n",
              " 'got': 189,\n",
              " 'thought': 190,\n",
              " 'big': 191,\n",
              " 'fact': 192,\n",
              " 'enough': 193,\n",
              " 'long': 194,\n",
              " 'both': 195,\n",
              " \"that's\": 196,\n",
              " 'give': 197,\n",
              " \"i've\": 198,\n",
              " 'own': 199,\n",
              " 'may': 200,\n",
              " 'between': 201,\n",
              " 'comedy': 202,\n",
              " 'right': 203,\n",
              " 'series': 204,\n",
              " 'action': 205,\n",
              " 'must': 206,\n",
              " 'music': 207,\n",
              " 'without': 208,\n",
              " 'times': 209,\n",
              " 'saw': 210,\n",
              " 'always': 211,\n",
              " 'original': 212,\n",
              " \"isn't\": 213,\n",
              " 'role': 214,\n",
              " 'come': 215,\n",
              " 'almost': 216,\n",
              " 'gets': 217,\n",
              " 'interesting': 218,\n",
              " 'guy': 219,\n",
              " 'point': 220,\n",
              " 'done': 221,\n",
              " \"there's\": 222,\n",
              " 'whole': 223,\n",
              " 'least': 224,\n",
              " 'far': 225,\n",
              " 'bit': 226,\n",
              " 'script': 227,\n",
              " 'minutes': 228,\n",
              " 'feel': 229,\n",
              " '2': 230,\n",
              " 'anything': 231,\n",
              " 'making': 232,\n",
              " 'might': 233,\n",
              " 'since': 234,\n",
              " 'am': 235,\n",
              " 'family': 236,\n",
              " \"he's\": 237,\n",
              " 'last': 238,\n",
              " 'probably': 239,\n",
              " 'tv': 240,\n",
              " 'performance': 241,\n",
              " 'kind': 242,\n",
              " 'away': 243,\n",
              " 'yet': 244,\n",
              " 'fun': 245,\n",
              " 'worst': 246,\n",
              " 'sure': 247,\n",
              " 'rather': 248,\n",
              " 'hard': 249,\n",
              " 'anyone': 250,\n",
              " 'girl': 251,\n",
              " 'each': 252,\n",
              " 'played': 253,\n",
              " 'day': 254,\n",
              " 'found': 255,\n",
              " 'looking': 256,\n",
              " 'woman': 257,\n",
              " 'screen': 258,\n",
              " 'although': 259,\n",
              " 'our': 260,\n",
              " 'especially': 261,\n",
              " 'believe': 262,\n",
              " 'having': 263,\n",
              " 'trying': 264,\n",
              " 'course': 265,\n",
              " 'dvd': 266,\n",
              " 'everything': 267,\n",
              " 'set': 268,\n",
              " 'goes': 269,\n",
              " 'comes': 270,\n",
              " 'put': 271,\n",
              " 'ending': 272,\n",
              " 'maybe': 273,\n",
              " 'place': 274,\n",
              " 'book': 275,\n",
              " 'shows': 276,\n",
              " 'three': 277,\n",
              " 'worth': 278,\n",
              " 'different': 279,\n",
              " 'main': 280,\n",
              " 'once': 281,\n",
              " 'sense': 282,\n",
              " 'american': 283,\n",
              " 'reason': 284,\n",
              " 'looks': 285,\n",
              " 'effects': 286,\n",
              " 'watched': 287,\n",
              " 'play': 288,\n",
              " 'true': 289,\n",
              " 'money': 290,\n",
              " 'actor': 291,\n",
              " \"wasn't\": 292,\n",
              " 'job': 293,\n",
              " 'together': 294,\n",
              " 'war': 295,\n",
              " 'someone': 296,\n",
              " 'plays': 297,\n",
              " 'instead': 298,\n",
              " 'high': 299,\n",
              " 'during': 300,\n",
              " 'said': 301,\n",
              " 'year': 302,\n",
              " 'half': 303,\n",
              " 'everyone': 304,\n",
              " 'later': 305,\n",
              " 'takes': 306,\n",
              " '1': 307,\n",
              " 'seem': 308,\n",
              " 'audience': 309,\n",
              " 'special': 310,\n",
              " 'beautiful': 311,\n",
              " 'left': 312,\n",
              " 'himself': 313,\n",
              " 'seeing': 314,\n",
              " 'john': 315,\n",
              " 'night': 316,\n",
              " 'black': 317,\n",
              " 'version': 318,\n",
              " 'shot': 319,\n",
              " 'excellent': 320,\n",
              " 'idea': 321,\n",
              " 'house': 322,\n",
              " 'mind': 323,\n",
              " 'star': 324,\n",
              " 'wife': 325,\n",
              " 'fan': 326,\n",
              " 'death': 327,\n",
              " 'used': 328,\n",
              " 'else': 329,\n",
              " 'simply': 330,\n",
              " 'nice': 331,\n",
              " 'budget': 332,\n",
              " 'poor': 333,\n",
              " 'short': 334,\n",
              " 'completely': 335,\n",
              " 'second': 336,\n",
              " \"you're\": 337,\n",
              " '3': 338,\n",
              " 'read': 339,\n",
              " 'less': 340,\n",
              " 'along': 341,\n",
              " 'top': 342,\n",
              " 'help': 343,\n",
              " 'home': 344,\n",
              " 'men': 345,\n",
              " 'either': 346,\n",
              " 'line': 347,\n",
              " 'boring': 348,\n",
              " 'dead': 349,\n",
              " 'friends': 350,\n",
              " 'kids': 351,\n",
              " 'try': 352,\n",
              " 'production': 353,\n",
              " 'enjoy': 354,\n",
              " 'camera': 355,\n",
              " 'use': 356,\n",
              " 'wrong': 357,\n",
              " 'given': 358,\n",
              " 'low': 359,\n",
              " 'classic': 360,\n",
              " 'father': 361,\n",
              " 'need': 362,\n",
              " 'full': 363,\n",
              " 'stupid': 364,\n",
              " 'next': 365,\n",
              " 'until': 366,\n",
              " 'performances': 367,\n",
              " 'school': 368,\n",
              " 'hollywood': 369,\n",
              " 'rest': 370,\n",
              " 'truly': 371,\n",
              " 'awful': 372,\n",
              " 'video': 373,\n",
              " 'couple': 374,\n",
              " 'start': 375,\n",
              " 'sex': 376,\n",
              " 'recommend': 377,\n",
              " 'women': 378,\n",
              " 'let': 379,\n",
              " 'tell': 380,\n",
              " 'terrible': 381,\n",
              " 'remember': 382,\n",
              " 'mean': 383,\n",
              " 'came': 384,\n",
              " 'getting': 385,\n",
              " 'understand': 386,\n",
              " 'perhaps': 387,\n",
              " 'moments': 388,\n",
              " 'name': 389,\n",
              " 'keep': 390,\n",
              " 'face': 391,\n",
              " 'itself': 392,\n",
              " 'wonderful': 393,\n",
              " 'playing': 394,\n",
              " 'human': 395,\n",
              " 'style': 396,\n",
              " 'small': 397,\n",
              " 'episode': 398,\n",
              " 'perfect': 399,\n",
              " 'others': 400,\n",
              " 'person': 401,\n",
              " 'doing': 402,\n",
              " 'often': 403,\n",
              " 'early': 404,\n",
              " 'stars': 405,\n",
              " 'definitely': 406,\n",
              " 'written': 407,\n",
              " 'head': 408,\n",
              " 'lines': 409,\n",
              " 'dialogue': 410,\n",
              " 'gives': 411,\n",
              " 'piece': 412,\n",
              " \"couldn't\": 413,\n",
              " 'went': 414,\n",
              " 'finally': 415,\n",
              " 'mother': 416,\n",
              " 'title': 417,\n",
              " 'case': 418,\n",
              " 'absolutely': 419,\n",
              " 'boy': 420,\n",
              " 'live': 421,\n",
              " 'yes': 422,\n",
              " 'laugh': 423,\n",
              " 'certainly': 424,\n",
              " 'liked': 425,\n",
              " 'become': 426,\n",
              " 'worse': 427,\n",
              " 'entertaining': 428,\n",
              " 'oh': 429,\n",
              " 'sort': 430,\n",
              " 'loved': 431,\n",
              " 'lost': 432,\n",
              " 'called': 433,\n",
              " 'hope': 434,\n",
              " 'picture': 435,\n",
              " 'felt': 436,\n",
              " 'overall': 437,\n",
              " 'entire': 438,\n",
              " 'several': 439,\n",
              " 'mr': 440,\n",
              " 'based': 441,\n",
              " 'supposed': 442,\n",
              " 'cinema': 443,\n",
              " 'friend': 444,\n",
              " 'guys': 445,\n",
              " 'sound': 446,\n",
              " '5': 447,\n",
              " 'problem': 448,\n",
              " 'drama': 449,\n",
              " 'against': 450,\n",
              " 'waste': 451,\n",
              " 'white': 452,\n",
              " 'beginning': 453,\n",
              " '4': 454,\n",
              " 'fans': 455,\n",
              " 'totally': 456,\n",
              " 'dark': 457,\n",
              " 'care': 458,\n",
              " 'direction': 459,\n",
              " 'humor': 460,\n",
              " 'wanted': 461,\n",
              " \"she's\": 462,\n",
              " 'seemed': 463,\n",
              " 'game': 464,\n",
              " 'under': 465,\n",
              " 'children': 466,\n",
              " 'despite': 467,\n",
              " 'lives': 468,\n",
              " 'lead': 469,\n",
              " 'guess': 470,\n",
              " 'example': 471,\n",
              " 'already': 472,\n",
              " 'final': 473,\n",
              " 'throughout': 474,\n",
              " \"you'll\": 475,\n",
              " 'turn': 476,\n",
              " 'evil': 477,\n",
              " 'becomes': 478,\n",
              " 'unfortunately': 479,\n",
              " 'able': 480,\n",
              " 'quality': 481,\n",
              " \"i'd\": 482,\n",
              " 'days': 483,\n",
              " 'history': 484,\n",
              " 'fine': 485,\n",
              " 'side': 486,\n",
              " 'wants': 487,\n",
              " 'heart': 488,\n",
              " 'horrible': 489,\n",
              " 'writing': 490,\n",
              " 'amazing': 491,\n",
              " 'b': 492,\n",
              " 'flick': 493,\n",
              " 'killer': 494,\n",
              " 'run': 495,\n",
              " 'son': 496,\n",
              " '\\x96': 497,\n",
              " 'michael': 498,\n",
              " 'works': 499,\n",
              " 'close': 500,\n",
              " \"they're\": 501,\n",
              " 'act': 502,\n",
              " 'art': 503,\n",
              " 'kill': 504,\n",
              " 'matter': 505,\n",
              " 'etc': 506,\n",
              " 'tries': 507,\n",
              " \"won't\": 508,\n",
              " 'past': 509,\n",
              " 'town': 510,\n",
              " 'enjoyed': 511,\n",
              " 'turns': 512,\n",
              " 'brilliant': 513,\n",
              " 'gave': 514,\n",
              " 'behind': 515,\n",
              " 'parts': 516,\n",
              " 'stuff': 517,\n",
              " 'genre': 518,\n",
              " 'eyes': 519,\n",
              " 'car': 520,\n",
              " 'favorite': 521,\n",
              " 'directed': 522,\n",
              " 'late': 523,\n",
              " 'hand': 524,\n",
              " 'expect': 525,\n",
              " 'soon': 526,\n",
              " 'hour': 527,\n",
              " 'obviously': 528,\n",
              " 'themselves': 529,\n",
              " 'sometimes': 530,\n",
              " 'killed': 531,\n",
              " 'actress': 532,\n",
              " 'thinking': 533,\n",
              " 'child': 534,\n",
              " 'girls': 535,\n",
              " 'viewer': 536,\n",
              " 'starts': 537,\n",
              " 'myself': 538,\n",
              " 'city': 539,\n",
              " 'decent': 540,\n",
              " 'highly': 541,\n",
              " 'stop': 542,\n",
              " 'type': 543,\n",
              " 'self': 544,\n",
              " 'god': 545,\n",
              " 'says': 546,\n",
              " 'group': 547,\n",
              " 'anyway': 548,\n",
              " 'voice': 549,\n",
              " 'took': 550,\n",
              " 'known': 551,\n",
              " 'blood': 552,\n",
              " 'kid': 553,\n",
              " 'heard': 554,\n",
              " 'happens': 555,\n",
              " 'except': 556,\n",
              " 'fight': 557,\n",
              " 'feeling': 558,\n",
              " 'experience': 559,\n",
              " 'coming': 560,\n",
              " 'slow': 561,\n",
              " 'daughter': 562,\n",
              " 'writer': 563,\n",
              " 'stories': 564,\n",
              " 'moment': 565,\n",
              " 'told': 566,\n",
              " 'leave': 567,\n",
              " 'extremely': 568,\n",
              " 'score': 569,\n",
              " 'violence': 570,\n",
              " 'police': 571,\n",
              " 'involved': 572,\n",
              " 'strong': 573,\n",
              " 'chance': 574,\n",
              " 'lack': 575,\n",
              " 'cannot': 576,\n",
              " 'hit': 577,\n",
              " 'hilarious': 578,\n",
              " 'roles': 579,\n",
              " 's': 580,\n",
              " 'wonder': 581,\n",
              " 'happen': 582,\n",
              " 'particularly': 583,\n",
              " 'ok': 584,\n",
              " 'including': 585,\n",
              " 'living': 586,\n",
              " 'save': 587,\n",
              " 'looked': 588,\n",
              " \"wouldn't\": 589,\n",
              " 'crap': 590,\n",
              " 'simple': 591,\n",
              " 'please': 592,\n",
              " 'cool': 593,\n",
              " 'murder': 594,\n",
              " 'obvious': 595,\n",
              " 'happened': 596,\n",
              " 'complete': 597,\n",
              " 'cut': 598,\n",
              " 'serious': 599,\n",
              " 'age': 600,\n",
              " 'gore': 601,\n",
              " 'attempt': 602,\n",
              " 'hell': 603,\n",
              " 'ago': 604,\n",
              " 'song': 605,\n",
              " 'shown': 606,\n",
              " 'taken': 607,\n",
              " 'english': 608,\n",
              " 'james': 609,\n",
              " 'robert': 610,\n",
              " 'david': 611,\n",
              " 'seriously': 612,\n",
              " 'released': 613,\n",
              " 'reality': 614,\n",
              " 'opening': 615,\n",
              " 'jokes': 616,\n",
              " 'interest': 617,\n",
              " 'across': 618,\n",
              " 'none': 619,\n",
              " 'hero': 620,\n",
              " 'today': 621,\n",
              " 'possible': 622,\n",
              " 'exactly': 623,\n",
              " 'alone': 624,\n",
              " 'sad': 625,\n",
              " 'brother': 626,\n",
              " 'number': 627,\n",
              " 'career': 628,\n",
              " 'saying': 629,\n",
              " \"film's\": 630,\n",
              " 'usually': 631,\n",
              " 'hours': 632,\n",
              " 'cinematography': 633,\n",
              " 'talent': 634,\n",
              " 'view': 635,\n",
              " 'annoying': 636,\n",
              " 'yourself': 637,\n",
              " 'running': 638,\n",
              " 'relationship': 639,\n",
              " 'documentary': 640,\n",
              " 'wish': 641,\n",
              " 'order': 642,\n",
              " 'huge': 643,\n",
              " 'whose': 644,\n",
              " 'shots': 645,\n",
              " 'ridiculous': 646,\n",
              " 'taking': 647,\n",
              " 'important': 648,\n",
              " 'light': 649,\n",
              " 'body': 650,\n",
              " 'middle': 651,\n",
              " 'level': 652,\n",
              " 'ends': 653,\n",
              " 'female': 654,\n",
              " 'call': 655,\n",
              " 'started': 656,\n",
              " \"i'll\": 657,\n",
              " 'husband': 658,\n",
              " 'four': 659,\n",
              " 'power': 660,\n",
              " 'turned': 661,\n",
              " 'major': 662,\n",
              " 'word': 663,\n",
              " 'opinion': 664,\n",
              " 'change': 665,\n",
              " 'mostly': 666,\n",
              " 'usual': 667,\n",
              " 'silly': 668,\n",
              " 'scary': 669,\n",
              " 'rating': 670,\n",
              " 'beyond': 671,\n",
              " 'somewhat': 672,\n",
              " 'happy': 673,\n",
              " 'ones': 674,\n",
              " 'words': 675,\n",
              " 'room': 676,\n",
              " 'knew': 677,\n",
              " 'knows': 678,\n",
              " 'country': 679,\n",
              " 'disappointed': 680,\n",
              " 'talking': 681,\n",
              " 'novel': 682,\n",
              " 'apparently': 683,\n",
              " 'non': 684,\n",
              " 'strange': 685,\n",
              " 'attention': 686,\n",
              " 'upon': 687,\n",
              " 'basically': 688,\n",
              " 'finds': 689,\n",
              " 'single': 690,\n",
              " 'cheap': 691,\n",
              " 'modern': 692,\n",
              " 'due': 693,\n",
              " 'jack': 694,\n",
              " 'television': 695,\n",
              " 'musical': 696,\n",
              " 'problems': 697,\n",
              " 'miss': 698,\n",
              " 'episodes': 699,\n",
              " 'clearly': 700,\n",
              " 'local': 701,\n",
              " '7': 702,\n",
              " 'british': 703,\n",
              " 'thriller': 704,\n",
              " 'talk': 705,\n",
              " 'events': 706,\n",
              " 'sequence': 707,\n",
              " 'five': 708,\n",
              " \"aren't\": 709,\n",
              " 'class': 710,\n",
              " 'french': 711,\n",
              " 'moving': 712,\n",
              " 'ten': 713,\n",
              " 'fast': 714,\n",
              " 'earth': 715,\n",
              " 'review': 716,\n",
              " 'tells': 717,\n",
              " 'predictable': 718,\n",
              " 'songs': 719,\n",
              " 'team': 720,\n",
              " 'comic': 721,\n",
              " 'straight': 722,\n",
              " 'whether': 723,\n",
              " '8': 724,\n",
              " 'die': 725,\n",
              " 'add': 726,\n",
              " 'dialog': 727,\n",
              " 'entertainment': 728,\n",
              " 'above': 729,\n",
              " 'sets': 730,\n",
              " 'future': 731,\n",
              " 'enjoyable': 732,\n",
              " 'appears': 733,\n",
              " 'near': 734,\n",
              " 'space': 735,\n",
              " 'easily': 736,\n",
              " 'hate': 737,\n",
              " 'soundtrack': 738,\n",
              " 'bring': 739,\n",
              " 'giving': 740,\n",
              " 'lots': 741,\n",
              " 'similar': 742,\n",
              " 'romantic': 743,\n",
              " 'george': 744,\n",
              " 'supporting': 745,\n",
              " 'release': 746,\n",
              " 'mention': 747,\n",
              " 'filmed': 748,\n",
              " 'within': 749,\n",
              " 'message': 750,\n",
              " 'sequel': 751,\n",
              " 'clear': 752,\n",
              " 'falls': 753,\n",
              " 'needs': 754,\n",
              " \"haven't\": 755,\n",
              " 'dull': 756,\n",
              " 'suspense': 757,\n",
              " 'bunch': 758,\n",
              " 'eye': 759,\n",
              " 'surprised': 760,\n",
              " 'showing': 761,\n",
              " 'tried': 762,\n",
              " 'sorry': 763,\n",
              " 'certain': 764,\n",
              " 'working': 765,\n",
              " 'easy': 766,\n",
              " 'ways': 767,\n",
              " 'theme': 768,\n",
              " 'theater': 769,\n",
              " 'among': 770,\n",
              " 'named': 771,\n",
              " \"what's\": 772,\n",
              " 'storyline': 773,\n",
              " 'monster': 774,\n",
              " 'king': 775,\n",
              " 'stay': 776,\n",
              " 'effort': 777,\n",
              " 'minute': 778,\n",
              " 'stand': 779,\n",
              " 'fall': 780,\n",
              " 'gone': 781,\n",
              " 'rock': 782,\n",
              " 'using': 783,\n",
              " '9': 784,\n",
              " 'feature': 785,\n",
              " 'comments': 786,\n",
              " 'buy': 787,\n",
              " \"'\": 788,\n",
              " 't': 789,\n",
              " 'typical': 790,\n",
              " 'sister': 791,\n",
              " 'editing': 792,\n",
              " 'tale': 793,\n",
              " 'avoid': 794,\n",
              " 'mystery': 795,\n",
              " 'deal': 796,\n",
              " 'dr': 797,\n",
              " 'doubt': 798,\n",
              " 'fantastic': 799,\n",
              " 'nearly': 800,\n",
              " 'kept': 801,\n",
              " 'feels': 802,\n",
              " 'subject': 803,\n",
              " 'okay': 804,\n",
              " 'viewing': 805,\n",
              " 'elements': 806,\n",
              " 'oscar': 807,\n",
              " 'check': 808,\n",
              " 'realistic': 809,\n",
              " 'points': 810,\n",
              " 'means': 811,\n",
              " 'greatest': 812,\n",
              " 'herself': 813,\n",
              " 'parents': 814,\n",
              " 'famous': 815,\n",
              " 'imagine': 816,\n",
              " 'rent': 817,\n",
              " 'viewers': 818,\n",
              " 'richard': 819,\n",
              " 'crime': 820,\n",
              " 'form': 821,\n",
              " 'peter': 822,\n",
              " 'actual': 823,\n",
              " 'lady': 824,\n",
              " 'general': 825,\n",
              " 'dog': 826,\n",
              " 'follow': 827,\n",
              " 'believable': 828,\n",
              " 'period': 829,\n",
              " 'red': 830,\n",
              " 'move': 831,\n",
              " 'brought': 832,\n",
              " 'material': 833,\n",
              " 'forget': 834,\n",
              " 'somehow': 835,\n",
              " 'begins': 836,\n",
              " 're': 837,\n",
              " 'reviews': 838,\n",
              " 'animation': 839,\n",
              " 'paul': 840,\n",
              " \"you've\": 841,\n",
              " 'leads': 842,\n",
              " 'weak': 843,\n",
              " 'figure': 844,\n",
              " 'surprise': 845,\n",
              " 'sit': 846,\n",
              " 'hear': 847,\n",
              " 'average': 848,\n",
              " 'open': 849,\n",
              " 'sequences': 850,\n",
              " 'killing': 851,\n",
              " 'atmosphere': 852,\n",
              " 'eventually': 853,\n",
              " 'tom': 854,\n",
              " 'learn': 855,\n",
              " 'premise': 856,\n",
              " 'wait': 857,\n",
              " '20': 858,\n",
              " 'sci': 859,\n",
              " 'deep': 860,\n",
              " 'fi': 861,\n",
              " 'expected': 862,\n",
              " 'whatever': 863,\n",
              " 'indeed': 864,\n",
              " 'note': 865,\n",
              " 'lame': 866,\n",
              " 'poorly': 867,\n",
              " 'particular': 868,\n",
              " 'imdb': 869,\n",
              " 'dance': 870,\n",
              " 'situation': 871,\n",
              " 'shame': 872,\n",
              " 'third': 873,\n",
              " 'box': 874,\n",
              " 'york': 875,\n",
              " 'truth': 876,\n",
              " 'decided': 877,\n",
              " 'free': 878,\n",
              " 'hot': 879,\n",
              " \"who's\": 880,\n",
              " 'difficult': 881,\n",
              " 'needed': 882,\n",
              " 'season': 883,\n",
              " 'acted': 884,\n",
              " 'leaves': 885,\n",
              " 'unless': 886,\n",
              " 'emotional': 887,\n",
              " 'possibly': 888,\n",
              " 'romance': 889,\n",
              " 'gay': 890,\n",
              " 'sexual': 891,\n",
              " 'boys': 892,\n",
              " 'footage': 893,\n",
              " 'write': 894,\n",
              " 'western': 895,\n",
              " 'credits': 896,\n",
              " 'forced': 897,\n",
              " 'memorable': 898,\n",
              " 'reading': 899,\n",
              " 'became': 900,\n",
              " 'doctor': 901,\n",
              " 'otherwise': 902,\n",
              " 'air': 903,\n",
              " 'crew': 904,\n",
              " 'de': 905,\n",
              " 'begin': 906,\n",
              " 'question': 907,\n",
              " 'meet': 908,\n",
              " 'society': 909,\n",
              " 'male': 910,\n",
              " 'meets': 911,\n",
              " \"let's\": 912,\n",
              " 'plus': 913,\n",
              " 'cheesy': 914,\n",
              " 'hands': 915,\n",
              " 'superb': 916,\n",
              " 'screenplay': 917,\n",
              " 'beauty': 918,\n",
              " 'interested': 919,\n",
              " 'street': 920,\n",
              " 'features': 921,\n",
              " 'masterpiece': 922,\n",
              " 'whom': 923,\n",
              " 'perfectly': 924,\n",
              " 'laughs': 925,\n",
              " 'stage': 926,\n",
              " 'nature': 927,\n",
              " 'effect': 928,\n",
              " 'forward': 929,\n",
              " 'comment': 930,\n",
              " 'nor': 931,\n",
              " 'e': 932,\n",
              " 'previous': 933,\n",
              " 'sounds': 934,\n",
              " 'badly': 935,\n",
              " 'japanese': 936,\n",
              " 'weird': 937,\n",
              " 'island': 938,\n",
              " 'personal': 939,\n",
              " 'inside': 940,\n",
              " 'quickly': 941,\n",
              " 'total': 942,\n",
              " 'keeps': 943,\n",
              " 'towards': 944,\n",
              " 'america': 945,\n",
              " 'result': 946,\n",
              " 'battle': 947,\n",
              " 'crazy': 948,\n",
              " 'worked': 949,\n",
              " 'setting': 950,\n",
              " 'incredibly': 951,\n",
              " 'background': 952,\n",
              " 'earlier': 953,\n",
              " 'mess': 954,\n",
              " 'cop': 955,\n",
              " 'writers': 956,\n",
              " 'fire': 957,\n",
              " 'copy': 958,\n",
              " 'unique': 959,\n",
              " 'realize': 960,\n",
              " 'dumb': 961,\n",
              " 'powerful': 962,\n",
              " 'lee': 963,\n",
              " 'mark': 964,\n",
              " 'business': 965,\n",
              " 'rate': 966,\n",
              " 'dramatic': 967,\n",
              " 'older': 968,\n",
              " 'pay': 969,\n",
              " 'following': 970,\n",
              " 'girlfriend': 971,\n",
              " 'directors': 972,\n",
              " 'joke': 973,\n",
              " 'plenty': 974,\n",
              " 'directing': 975,\n",
              " 'various': 976,\n",
              " 'baby': 977,\n",
              " 'creepy': 978,\n",
              " 'development': 979,\n",
              " 'appear': 980,\n",
              " 'brings': 981,\n",
              " 'front': 982,\n",
              " 'dream': 983,\n",
              " 'ask': 984,\n",
              " 'water': 985,\n",
              " 'admit': 986,\n",
              " 'rich': 987,\n",
              " 'bill': 988,\n",
              " 'apart': 989,\n",
              " 'joe': 990,\n",
              " 'fairly': 991,\n",
              " 'political': 992,\n",
              " 'leading': 993,\n",
              " 'reasons': 994,\n",
              " 'portrayed': 995,\n",
              " 'spent': 996,\n",
              " 'telling': 997,\n",
              " 'cover': 998,\n",
              " 'outside': 999,\n",
              " 'fighting': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZGGeitPt2WT"
      },
      "source": [
        "Tokenizer로 훈련 데이터의 텍스트를 정수로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sOOcMmI3t2WT"
      },
      "outputs": [],
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSI5KoKwt2WT"
      },
      "source": [
        "For example, here is a text from the training-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bD7v-6HVt2WU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e01441-ba36-426a-bb55-845fe8354ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probably the first Portuguese film I have seen in my life, and I enjoyed it. The plot is related of how the young army officers took the power in Portugal in 1974, to finally defeat the fascist government of Caetano and to also finalize the wars in the colonies, i.e. Mozambique, Angola, and Guinea (Bissau)- Cape Vert. Most of the events shown in the film reflect with exactitude the behavior of the army officers and soldiers to conduct the coup, of the oppressed people, who were very happy with this new development and the liberty, the resistance of Caetano's men, and also in a subtle way of most conservative officials, including Spinola, who took over as the new president. The Portuguese revolution can be remembered because of the action of several young officers, but for me the most interesting part of the film was when the young captain expressed that Portugal should develop itself democratically, and this is what the country achieved some years after this coup or revolution. The film also shows that the army officers and soldiers never wanted to kill anyone; even the most serious enemies were respected at the end.\n"
          ]
        }
      ],
      "source": [
        "# TODO: 훈련 데이터의 0번째 원본 텍스트(x_train_text) 출력해보기\n",
        "print(x_train_text[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xVyrza1ct2WU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3b4733-14de-4d43-c9de-59eac350a4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[239, 1, 86, 7658, 19, 10, 25, 107, 8, 56, 114, 2, 10, 511, 9, 1, 111, 6, 2469, 4, 85, 1, 186, 1248, 3854, 550, 1, 660, 8, 8, 5628, 5, 415, 4399, 1, 7659, 1310, 4, 2, 5, 81, 1, 1698, 8, 1, 10, 932, 2, 7888, 7364, 88, 4, 1, 706, 606, 8, 1, 19, 4423, 16, 1, 1932, 4, 1, 1248, 3854, 2, 1246, 5, 9498, 1, 7479, 4, 1, 83, 35, 70, 52, 673, 16, 11, 168, 979, 2, 1, 7365, 1, 7366, 4, 345, 2, 81, 8, 3, 1284, 95, 4, 88, 4679, 7540, 585, 35, 550, 121, 14, 1, 168, 1998, 1, 7658, 2938, 67, 26, 2025, 84, 4, 1, 205, 4, 439, 186, 3854, 18, 15, 68, 1, 88, 218, 173, 4, 1, 19, 13, 50, 1, 186, 1622, 4881, 12, 142, 2238, 392, 2, 11, 6, 48, 1, 679, 3225, 47, 153, 100, 11, 7479, 38, 2938, 1, 19, 81, 276, 12, 1, 1248, 3854, 2, 1246, 110, 461, 5, 504, 250, 57, 1, 88, 599, 4180, 70, 4780, 30, 1, 127]\n"
          ]
        }
      ],
      "source": [
        "# TODO: 위에서 정수로 변환한 0번째 텍스트 출력해보기\n",
        "# first는 86, of는 4, all은 29로 변환됐는지 살펴보세요.\n",
        "print(x_train_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvOL1jHAt2WU"
      },
      "source": [
        "We also need to convert the texts in the test-set to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FAjPUecQt2WU"
      },
      "outputs": [],
      "source": [
        "# TODO: tokenizer로 전체 테스트 데이터를 정수로 변환해보세요.\n",
        "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHGyZPoEt2WU"
      },
      "source": [
        "## Padding and Truncating Data\n",
        "\n",
        "순환신경망은 임의의 길이를 지닌 입력을 처리할 수 있지만 훈련의 속도를 높이기 위해 동일 길이를 지닌 입력 데이터의 묶음 (batch) 단위로 훈련이 이루어지는 관계로 편의상 모든 입력 데이터의 길이를 동일하게 합니다. 모든 입력 데이터의 길이를 동일하게 하는 방법은 크게 두 가지가 있습니다.\n",
        "\n",
        "1. 가장 길이가 긴 리뷰의 길이에 맞추어 다른 리뷰들의 앞, 혹은 뒤에 공백을 삽입한다.\n",
        "\n",
        "2. 적당한 길이를 정한다음 길이가 긴 리뷰는 앞, 혹은 뒤를 잘라내고 짧은 리뷰는 앞, 혹은 뒤에 공백을 삽입한다.\n",
        "\n",
        "첫 번째 방법은 지나치게 긴 리뷰가 있을 경우 다른 리뷰들에 지나치게 많은 공백이 들어가 데이터셋이 커져 훈련하는데 오래 걸리고 메모리의 낭비도 심해지는 단점이 있어 우리는 두 번째 방법을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RlDFg_w0t2WU"
      },
      "outputs": [],
      "source": [
        "# 각 리뷰에 단어 (토큰)가 몇 개씩 들어있는지 세어봅시다.\n",
        "\n",
        "num_tokens = [] # 빈 리스트 생성\n",
        "\n",
        "# TODO: num_tokens에 각 리뷰의 단어 숫자를 저장하세요.\n",
        "# num_tokens 원소는 50,000개이며 처음 25,000은 훈련 데이터의 단어숫자가 저장되어 있고,\n",
        "# 나머지 25,000은 테스트 데이터의 단어숫자가 저장되어 있습니다.\n",
        "# Hint: x_train_tokens[0]은 첫 번째 훈련 데이터의 단어들이 저장되어 있습니다. 그 길이는 len 함수로 알 수 있습니다.\n",
        "for i in range(len(x_train_tokens)):\n",
        "  num_tokens.append(len(x_train_tokens[i]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: num_tokens 리스트를 num_tokens NumPy 배열로 변환하세요.\n",
        "num_tokens = np.array(num_tokens)"
      ],
      "metadata": {
        "id": "9kwihq8yLf6H"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYdxLK99t2WU"
      },
      "source": [
        "The average number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "S0kuTohct2WV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735eae55-8131-40a8-95d7-42cbc9ab33ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "223.73656"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# 모든 리뷰에서 사용한 평균 단어의 수는 다음과 같습니다.\n",
        "np.mean(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDu36sCDt2WV"
      },
      "source": [
        "The maximum number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IKujyc3It2WV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd031f73-1e7a-4daf-8162-7709f49f6458"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2142"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# TODO: 가장 긴 리뷰의 단어수를 출력해보세요.\n",
        "# Hint: np.max\n",
        "np.max(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 리뷰에 사용된 단어수(토큰수)에 따른 리뷰의 수 살펴보기\n",
        "# 대부분의 리뷰가 500개 이하의 토큰만 사용했음을 알 수 있음\n",
        "plt.hist(num_tokens, bins=50)\n",
        "plt.xlabel('Number of tokens')\n",
        "plt.ylabel('Number of reviews')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BzLYeiVRBXc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "b8dac7f0-f3ab-4d1e-eb41-18a463901606"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwAj_aOIt2WV"
      },
      "source": [
        "The max number of tokens we will allow is set to the average plus 2 standard deviations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_M8wDkUJt2WV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f073e57d-a18f-43b9-aeeb-bfd4e8a67128"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "551"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# TODO: num_tokens에서 평균 + 2 * 표준편차를 구하여 우리가 허용할 최대 단어의 개수인 max_tokens를 설정해보세요.\n",
        "# num_tokens가 정규분포를 따른다면 97% 이상이 max_tokens보다 적은 단어만을 사용할 것입니다.\n",
        "# Hint: np.mean과 np.std 사용\n",
        "\n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B34-DJ_ot2WV"
      },
      "source": [
        "This covers about 95% of the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Vg-lkIWjt2WV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd5179a-a5e4-4d7f-8195-4c3cd8142184"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94508"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# max_tokens보다 짧은 리뷰의 비율을 출력하는 코드\n",
        "np.sum(num_tokens < max_tokens) / len(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리뷰의 94% 이상은 max_tokens보다 짧으므로 공백을 삽입하여 길이를 max_tokens로 맞춰주고, max_tokens보다 긴 리뷰의 길이는 max_tokens로 자를 필요가 있습니다. 모든 리뷰를 가장 길었던 리뷰의 길이로 맞췄다면 데이터셋 크기가 굉장히 커졌을 것입니다.<br>\n",
        "\n",
        "공백을 삽입할 때 리뷰의 앞에 ('pre') 삽입할 수도 있고 마지막 ('post')에 삽입할 수도 있는데 인공신경망에 혼란을 주지 않기 위해 일관된 방식으로 삽입해야 합니다. (자르기도 동일)"
      ],
      "metadata": {
        "id": "cX2jx6eINEJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "M8Y_JY2St2WW"
      },
      "outputs": [],
      "source": [
        "# 공백을 리뷰의 앞에 삽입\n",
        "pad = 'pre'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XkY-yQDgt2WW"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터의 길이를 max_tokens로 맞춰주기\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,  # 입력의 최대 길이는 max_tokens\n",
        "                            padding=pad, truncating=pad)        # 패딩과 자르기 모두 리뷰의 앞부분에서 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "okwoNpgGt2WW"
      },
      "outputs": [],
      "source": [
        "# TODO: 테스트 데이터의 길이를 max_tokens로 맞춰주세요.\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens, padding=pad, truncating=pad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCKTP6vPt2WW"
      },
      "source": [
        "We have now transformed the training-set into one big matrix of integers (tokens) with this shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LVp4YkW7t2WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d64117-0ac6-41d9-ddf8-d83ed8437cad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 551)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# 훈련 데이터: (리뷰 개수, 각 리뷰의 길이)\n",
        "x_train_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrnD4H3Zt2WW"
      },
      "source": [
        "The matrix for the test-set has the same shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "scrolled": true,
        "id": "qJRJa6rgt2WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a3dea8-fb6d-4fa0-89ee-612c288d8607"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 551)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# 테스트 데이터: (리뷰 개수, 각 리뷰의 길이)\n",
        "x_test_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxij8nRt2WW"
      },
      "source": [
        "For example, we had the following sequence of tokens above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "LjIbxctIt2WX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298eff03-41cc-4cbe-c938-51ba27b95a73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 239,    1,   86, 7658,   19,   10,   25,  107,    8,   56,  114,\n",
              "          2,   10,  511,    9,    1,  111,    6, 2469,    4,   85,    1,\n",
              "        186, 1248, 3854,  550,    1,  660,    8,    8, 5628,    5,  415,\n",
              "       4399,    1, 7659, 1310,    4,    2,    5,   81,    1, 1698,    8,\n",
              "          1,   10,  932,    2, 7888, 7364,   88,    4,    1,  706,  606,\n",
              "          8,    1,   19, 4423,   16,    1, 1932,    4,    1, 1248, 3854,\n",
              "          2, 1246,    5, 9498,    1, 7479,    4,    1,   83,   35,   70,\n",
              "         52,  673,   16,   11,  168,  979,    2,    1, 7365,    1, 7366,\n",
              "          4,  345,    2,   81,    8,    3, 1284,   95,    4,   88, 4679,\n",
              "       7540,  585,   35,  550,  121,   14,    1,  168, 1998,    1, 7658,\n",
              "       2938,   67,   26, 2025,   84,    4,    1,  205,    4,  439,  186,\n",
              "       3854,   18,   15,   68,    1,   88,  218,  173,    4,    1,   19,\n",
              "         13,   50,    1,  186, 1622, 4881,   12,  142, 2238,  392,    2,\n",
              "         11,    6,   48,    1,  679, 3225,   47,  153,  100,   11, 7479,\n",
              "         38, 2938,    1,   19,   81,  276,   12,    1, 1248, 3854,    2,\n",
              "       1246,  110,  461,    5,  504,  250,   57,    1,   88,  599, 4180,\n",
              "         70, 4780,   30,    1,  127])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# 패딩 전의 0번째 훈련 데이터\n",
        "np.array(x_train_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVbARkK0t2WX"
      },
      "source": [
        "This has simply been padded to create the following sequence. Note that when this is input to the Recurrent Neural Network, then it first inputs a lot of zeros. If we had padded 'post' then it would input the integer-tokens first and then a lot of zeros. This may confuse the Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WvblmFmht2WX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6cd0de-753b-40f2-d479-1636e1c81549"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,  239,    1,   86, 7658,\n",
              "         19,   10,   25,  107,    8,   56,  114,    2,   10,  511,    9,\n",
              "          1,  111,    6, 2469,    4,   85,    1,  186, 1248, 3854,  550,\n",
              "          1,  660,    8,    8, 5628,    5,  415, 4399,    1, 7659, 1310,\n",
              "          4,    2,    5,   81,    1, 1698,    8,    1,   10,  932,    2,\n",
              "       7888, 7364,   88,    4,    1,  706,  606,    8,    1,   19, 4423,\n",
              "         16,    1, 1932,    4,    1, 1248, 3854,    2, 1246,    5, 9498,\n",
              "          1, 7479,    4,    1,   83,   35,   70,   52,  673,   16,   11,\n",
              "        168,  979,    2,    1, 7365,    1, 7366,    4,  345,    2,   81,\n",
              "          8,    3, 1284,   95,    4,   88, 4679, 7540,  585,   35,  550,\n",
              "        121,   14,    1,  168, 1998,    1, 7658, 2938,   67,   26, 2025,\n",
              "         84,    4,    1,  205,    4,  439,  186, 3854,   18,   15,   68,\n",
              "          1,   88,  218,  173,    4,    1,   19,   13,   50,    1,  186,\n",
              "       1622, 4881,   12,  142, 2238,  392,    2,   11,    6,   48,    1,\n",
              "        679, 3225,   47,  153,  100,   11, 7479,   38, 2938,    1,   19,\n",
              "         81,  276,   12,    1, 1248, 3854,    2, 1246,  110,  461,    5,\n",
              "        504,  250,   57,    1,   88,  599, 4180,   70, 4780,   30,    1,\n",
              "        127], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# TODO: 패딩 후의 0번째 훈련 데이터를 출력해보세요.\n",
        "# Hint: 바로 위와 다른 형태로 나와야 합니다.\n",
        "np.array(x_train_pad[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuEuwTb7t2WX"
      },
      "source": [
        "## Tokenizer Inverse Map\n",
        "\n",
        "정수의 배열을 다시 텍스트로 바꿔주는 함수를 만들어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GcQFzRXkt2WX"
      },
      "outputs": [],
      "source": [
        "idx = tokenizer.word_index # tokenizer는 단어 (key)-> 정수 (value)이므로\n",
        "inverse_map = dict(zip(idx.values(), idx.keys())) # 정수 (value) -> 단어 (key)로 보내면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUsOFMx3t2WX"
      },
      "source": [
        "Helper-function for converting a list of tokens back to a string of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CmNXSZX3t2WX"
      },
      "outputs": [],
      "source": [
        "# tokens_to_string 함수에 정수 배열을 입력하면 텍스트로 출력됩니다.\n",
        "def tokens_to_string(tokens):\n",
        "    # Map from tokens back to words.\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "\n",
        "    # Concatenate all words.\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8joKACPJt2WX"
      },
      "source": [
        "For example, this is the original text from the data-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "scrolled": true,
        "id": "G7-Rex8ot2WX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "83830102-6cfa-4aa1-cb8c-db7372e78fad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Probably the first Portuguese film I have seen in my life, and I enjoyed it. The plot is related of how the young army officers took the power in Portugal in 1974, to finally defeat the fascist government of Caetano and to also finalize the wars in the colonies, i.e. Mozambique, Angola, and Guinea (Bissau)- Cape Vert. Most of the events shown in the film reflect with exactitude the behavior of the army officers and soldiers to conduct the coup, of the oppressed people, who were very happy with this new development and the liberty, the resistance of Caetano's men, and also in a subtle way of most conservative officials, including Spinola, who took over as the new president. The Portuguese revolution can be remembered because of the action of several young officers, but for me the most interesting part of the film was when the young captain expressed that Portugal should develop itself democratically, and this is what the country achieved some years after this coup or revolution. The film also shows that the army officers and soldiers never wanted to kill anyone; even the most serious enemies were respected at the end.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# 훈련 데이터의 0번째 리뷰 실제 텍스트\n",
        "x_train_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수로 변환된 훈련 데이터의 0번째 리뷰\n",
        "np.array(x_train_tokens[0])"
      ],
      "metadata": {
        "id": "1q-SfSwdQQS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19d37d4-e778-4ce6-a655-f48f0e5f89c4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 239,    1,   86, 7658,   19,   10,   25,  107,    8,   56,  114,\n",
              "          2,   10,  511,    9,    1,  111,    6, 2469,    4,   85,    1,\n",
              "        186, 1248, 3854,  550,    1,  660,    8,    8, 5628,    5,  415,\n",
              "       4399,    1, 7659, 1310,    4,    2,    5,   81,    1, 1698,    8,\n",
              "          1,   10,  932,    2, 7888, 7364,   88,    4,    1,  706,  606,\n",
              "          8,    1,   19, 4423,   16,    1, 1932,    4,    1, 1248, 3854,\n",
              "          2, 1246,    5, 9498,    1, 7479,    4,    1,   83,   35,   70,\n",
              "         52,  673,   16,   11,  168,  979,    2,    1, 7365,    1, 7366,\n",
              "          4,  345,    2,   81,    8,    3, 1284,   95,    4,   88, 4679,\n",
              "       7540,  585,   35,  550,  121,   14,    1,  168, 1998,    1, 7658,\n",
              "       2938,   67,   26, 2025,   84,    4,    1,  205,    4,  439,  186,\n",
              "       3854,   18,   15,   68,    1,   88,  218,  173,    4,    1,   19,\n",
              "         13,   50,    1,  186, 1622, 4881,   12,  142, 2238,  392,    2,\n",
              "         11,    6,   48,    1,  679, 3225,   47,  153,  100,   11, 7479,\n",
              "         38, 2938,    1,   19,   81,  276,   12,    1, 1248, 3854,    2,\n",
              "       1246,  110,  461,    5,  504,  250,   57,    1,   88,  599, 4180,\n",
              "         70, 4780,   30,    1,  127])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya16yBQrt2WY"
      },
      "source": [
        "We can recreate this text except for punctuation and other symbols, by converting the list of tokens back to words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "DqDwOJ9Lt2WY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "421f3078-b4bb-4d6d-a59e-0764228b08f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'probably the first portuguese film i have seen in my life and i enjoyed it the plot is related of how the young army officers took the power in in 1974 to finally defeat the fascist government of and to also the wars in the i e and guinea cape most of the events shown in the film reflect with the behavior of the army officers and soldiers to conduct the coup of the people who were very happy with this new development and the liberty the resistance of men and also in a subtle way of most conservative officials including who took over as the new president the portuguese revolution can be remembered because of the action of several young officers but for me the most interesting part of the film was when the young captain expressed that should develop itself and this is what the country achieved some years after this coup or revolution the film also shows that the army officers and soldiers never wanted to kill anyone even the most serious enemies were respected at the end'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# TODO: 정수 형태의 0번째 리뷰를 다시 단어로 변환해보세요.\n",
        "# 모두 소문자로 변경되고, 잘 쓰이지 않는 단어들은 삭제된 상태입니다.\n",
        "# Hint: 위에서 정의한 tokens_to_string 함수 이용\n",
        "tokens_to_string(x_train_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hamblZDft2WY"
      },
      "source": [
        "## Create the Recurrent Neural Network\n",
        "\n",
        "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial #03-C for a tutorial on Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8nObzn4ft2WY"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggam4ai4t2WY"
      },
      "source": [
        "자연어 처리를 위한 순환 신경망의 가장 첫 번째 층은 임베딩층입니다. 우리가 선택한 10000개의 단어를 우리가 지정한 차원에 보내 비슷한 단어가 가까운 곳에 위치하도록 만들 것입니다. (Tokenizer가 만든 정수는 1: the, 2: and, 3: a, 4: of과 같이 뜻이 비슷하지도 않은데 단어들이 가까운 곳에 위치해 있습니다.)\n",
        "\n",
        "일반적으로 임베딩 차원은 100에서 300 정도를 사용하는데 일단 8차원만 사용해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ObHSK_FKt2WY"
      },
      "outputs": [],
      "source": [
        "embedding_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnOGpWR8t2WY"
      },
      "source": [
        "The embedding-layer also needs to know the number of words in the vocabulary (`num_words`) and the length of the padded token-sequences (`max_tokens`). We also give this layer a name because we need to retrieve its weights further below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sVBLbYdrt2WY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6f02f5-0994-4506-cd0d-ae6096115e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 임베딩층 추가\n",
        "model.add(Embedding(input_dim=num_words,      # 사용하는 단어의 개수\n",
        "                    output_dim=embedding_size,# 임베딩 차원\n",
        "                    input_length=max_tokens,  # 리뷰의 길이\n",
        "                    name='layer_embedding'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36FqWbR9t2WY"
      },
      "source": [
        "순환 유닛인 GRU를 추가하고 Dense로 묶어 하나의 출력에서 0과 1사이의 실수를 출력하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "LnuPpp4zt2WY"
      },
      "outputs": [],
      "source": [
        "model.add(GRU(units=256))\n",
        "model.add(Dense(1, activation='sigmoid')) # 이전 층의 입력을 받아들여 0부터 1사이의 실수값 하나 출력 (sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUlN1Vxvt2WZ"
      },
      "source": [
        "Use the Adam optimizer with the given learning-rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "h9tJRHtZt2WZ"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kah4uUD3t2WZ"
      },
      "source": [
        "Compile the Keras model so it is ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "KRiczqCyt2WZ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZINq_6Bkt2Wa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "dbe0d9bc-b321-4708-f301-f9643712c145"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ layer_embedding (\u001b[38;5;33mEmbedding\u001b[0m)          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ layer_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZYcJVRpt2Wa"
      },
      "source": [
        "## Train the Recurrent Neural Network\n",
        "\n",
        "We can now train the model. Note that we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps over-fitting to the training-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "scrolled": true,
        "id": "tQxdKqGbt2Wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d45a67a-aaec-4831-8cf5-afb649abbcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 38ms/step - accuracy: 0.6303 - loss: 0.6557 - val_accuracy: 0.7654 - val_loss: 0.5876\n",
            "Epoch 2/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - accuracy: 0.8353 - loss: 0.3851 - val_accuracy: 0.7834 - val_loss: 0.5309\n",
            "Epoch 3/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - accuracy: 0.8861 - loss: 0.2825 - val_accuracy: 0.8342 - val_loss: 0.4840\n",
            "Epoch 4/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9086 - loss: 0.2345 - val_accuracy: 0.8192 - val_loss: 0.4639\n",
            "Epoch 5/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9323 - loss: 0.1879 - val_accuracy: 0.7922 - val_loss: 0.5595\n",
            "Epoch 6/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.9477 - loss: 0.1486 - val_accuracy: 0.8542 - val_loss: 0.4069\n",
            "Epoch 7/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.9539 - loss: 0.1310 - val_accuracy: 0.8004 - val_loss: 0.6126\n",
            "Epoch 8/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.9622 - loss: 0.1149 - val_accuracy: 0.8424 - val_loss: 0.4878\n",
            "Epoch 9/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.9683 - loss: 0.0956 - val_accuracy: 0.6948 - val_loss: 1.0435\n",
            "Epoch 9: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b6840df6f50>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# validation을 통해 테스트셋에서의 성능을 가늠할 수 있습니다.\n",
        "# 훈련데이터에 대한 성능은 개선되는데 validation에서 악화된다면 과적합이 일어나는 것일 수도 있습니다.\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# TODO: 자유롭게 설정, restore_best_weights=True 추천\n",
        "es = EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True,verbose=1)\n",
        "\n",
        "model.fit(x_train_pad, y_train, callbacks=es,\n",
        "          validation_split=0.2, epochs=10000, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "382P8ZGMt2Wa"
      },
      "source": [
        "## Performance on Test-Set\n",
        "\n",
        "Now that the model has been trained we can calculate its classification accuracy on the test-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hP1eEs8xt2Wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ad9263-e945-4b18-d3fe-1da94173e1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8570 - loss: 0.3778\n",
            "CPU times: user 7.45 s, sys: 315 ms, total: 7.76 s\n",
            "Wall time: 9.33 s\n"
          ]
        }
      ],
      "source": [
        "# 테스트셋에서의 성능 평가\n",
        "%%time\n",
        "result = model.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alpR4ZXut2Wc"
      },
      "source": [
        "## New Data\n",
        "\n",
        "Let us try and classify new texts that we make up. Some of these are obvious, while others use negation and sarcasm to try and confuse the model into mis-classifying the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "7dguFHY5t2Wc"
      },
      "outputs": [],
      "source": [
        "# 새로운 데이터셋 8개 생성하여 texts에 저장\n",
        "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "text2 = \"Good movie!\"\n",
        "text3 = \"Maybe I like this movie.\"\n",
        "text4 = \"Meh ...\"\n",
        "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
        "text6 = \"Bad movie!\"\n",
        "text7 = \"Not a good movie!\"\n",
        "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
        "texts = [text1, text2, text3, text4, text5, text6, text7, text8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L21c2SlFt2Wc"
      },
      "source": [
        "We first convert these texts to arrays of integer-tokens because that is needed by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "N53OFtJxt2Wd"
      },
      "outputs": [],
      "source": [
        "# TODO: tokenizer로 texts를 정수로 변환하세요.\n",
        "tokens = tokenizer.texts_to_sequences(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tX0cSgCt2Wd"
      },
      "source": [
        "To input texts with different lengths into the model, we also need to pad and truncate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "gCZytuX-t2Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17fa242-8f71-43f5-919a-c8c8949ac746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 551)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# TODO: 패딩으로 tokens의 길이를 맞춰주세요.\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "tokens_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPwl4x3nt2Wd"
      },
      "source": [
        "We can now use the trained model to predict the sentiment for these texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "5smpP0MOt2Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3652b6-72fe-4447-81e0-26b5ff088692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98931515],\n",
              "       [0.999418  ],\n",
              "       [0.9549755 ],\n",
              "       [1.        ],\n",
              "       [0.7566327 ],\n",
              "       [0.9816062 ],\n",
              "       [0.9893595 ],\n",
              "       [0.06901944]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# TODO: model로 tokens_pad를 예측해보세요.\n",
        "# 1에 가까울 수록 긍정적이고 0에 가까울 수록 부정적입니다.\n",
        "# 잘 예측했나요? 운에 따라 결과가 다를 수 있습니다.\n",
        "model.predict(tokens_pad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYhK-DEot2Wd"
      },
      "source": [
        "A value close to 0.0 means a negative sentiment and a value close to 1.0 means a positive sentiment. These numbers will vary every time you train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newer Data\n",
        "우리 모델은 평균 200개 이상의 단어가 쓰인 리뷰로 학습됐습니다. 위와 같이 매우 짧은 리뷰에 대해서는 잘 동작하지 않을 수도 있으니 <br>IMDb에서 듄 Part Two의 10점짜리 리뷰와 5점 이하의 리뷰로 테스트 해봅시다.<br>\n",
        "https://www.imdb.com/title/tt15239678/reviews"
      ],
      "metadata": {
        "id": "aEL_WlY7HzWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Had the pleasure to watch this film in an early screening and was completely blown away. Dune: Part 2 is everything one could ask for from a film of its kind. As a big fan of the Game of Thrones franchise, it's been a long time since iv'e encountered this level of world-building and epicness. The plot and story development are carried out in an awe-inspiring manner throughout the movie, progressing at a precise pace toward a spectacular climax that is executed perfectly. Denis Villeneuve continues to prove himself as one of the most promising filmmakers of our time, and if it was up to me I would keep him in these high-budget epic tales such as these since there are very few directors working today that can tackle this genre as good as he does. The film received praise from many great filmmakers, the most notable being Christopher Nolan (The Dark Knight Trilogy, Oppenheimer), who very accurately compared Villeneuve's achievement in this film to the Empire Strikes Back of the modern era. Would highly recommend to re-watch the first movie in order to appreciate the subtleties and foreshadowing better, though it's not completely necessary since the movie is pretty self-contained. More specifically, one of the most prominent and impressive strengths of the movie is how it stands on its own without relying too heavily on the first part. Of course, for those who really want to enjoy the story, there's no real point in watching the second part without having seen the first one. However, even those who watched it a few years ago and don't remember every little plot detail will be able to catch up very quickly while watching the Pt. 2 and will understand all the important details-even the smaller ones. Non-ideally, even someone who hasn't watched the first movie at all will greatly enjoy this one, as it is very self-contained despite relying heavily on the first movie-an impressive and highly commendable screenplay effort by the writers. Timothée Chalamet delivers a very impressive and charismatic performance, to the point where it seems like this is the role he was born to play. Zendaya also gives a very good performance, with significantly more screen time than in the first movie. Austin Butler manages to be terrifying in the role of the villain, Rebecca Ferguson shines in one of the best performances of the year, and Javier Bardem surprises in a role that sometimes provides the comic relief of the movie. In addition to them, the film is filled with top-tier actors and actresses who all perform their roles amazingly. Hans Zimmer's score is masterful just as you'd expect, and one step up from the Oscar winning and Grammy nominated score of Pt. 1. Technical aspects such as VFX, Production Design, Sound, Editing, etc. Are all top notch and awards-worthy. The action sequences are absolutely mind blowing and sent chills down my spine. Denis direction is impeccable, and the story is absolutely fascinating- continuing to develop characters from Pt. 1 even deeper and introduce new incredible and intriguing characters. In my personal opinion, the movie is better than the first part in pretty much every aspect. While the first part was excellent, it mainly served as a setup and positioning of the pieces for the sequel. In Part 2, the story reaches its climax, with one of the strongest climaxes seen on the big screen in recent decades, befitting the complex work Frank Herbert wrote many years ago. I believe (and surely hope) this movie will be a major player in the next Award Season. Below-the-line wins are pretty much guaranteed as of now, but I hope it will get some love in above-the-line categories such as Direction and even Best Picture, perhaps to break stigmas presented against the Fantasy/Sci-Fi genre in recent years at the Academy. As a big fan of the genre, unless a better competitor will be released later this eligibility period, maybe it's time for a movie like this to triumph once again. While the movie serves as a great wrap-up to the story introduced in Pt. 1, in my opinion it could serve as an even better set-up to a possible masterpiece in Dune: Messiah, which I really hope will get green-lit soon. Don't miss the opportunity to catch this movie on IMAX, since I believe it's an historic piece of epic-Fantasy/Sci-Fi cinema and a movie that will be remembered as a classic of the genre. Extremely recommended.\"\n",
        "text2 = \"If you liked or loved the first one, the same will apply for this one. Personally, I loved this one even more and I think general audiences will as well. So I hope it does well at the box office because I need Dune Messiah now. This is everything I love about going to the movies. Also, Hans Zimmer.For book readers, I'd say this one takes more liberties than the first, but they were changes I liked, some for the better even. For the most part, it still hits the main beats from the book with a few things altered. The biggest change being no time jump. Therefore, a certain character doesn't fully appear in the movie contrary to the book. Though, they still use the character in a different way that I liked personally. I know it's early in the year, but you can lock some Oscar nominations for this movie: Best Picture, Best Director, Best Adapted Screenplay, Best Editing, Best Cinematography, Best Score, Best Sound, Best Visual Effects, Best Costume, Best Production Design, Best Makeup, and hopefully Rebecca Ferguson this time for Best Supporting Actress. I guess that's pretty much every category now that I've typed it out lol.\"\n",
        "text3 = \"This is one of those films that is overrated by a whole 5 stars because people who don't get it or fall asleep blame themselves rather than the film. Their inclination is to follow the herd... or the emperor in this case. Only problem is that the emperor has no clothes. It's a cultural phenomena whereby if something seems or has the pretence of being intelligent, half the population is scared to call it out as dumb. It's just a bunch of nonsense art-house type scenes thrown together without any particular order or purpose.. and they could do some decent CGI because, thanks to the abundance of sand, there is really very little on screen at any one time. There's really nothing else here and nothing that works.\"\n",
        "text4 = \"It's the worst movie I've watched in a long time. If someone who never saw a previous 1984 film or read a novel watches this one would not understand a thing because there is no logical explanation between some of the events like when Paul's sister appeared all of a sudden and no explanation how or why. The movie itsef is very slow and boring. The main actor looks like a whiney teen aristocrat instead of a strong young man. Even physically he doesn't look like someone who can stand a real fight or lead the whole nation. In the novel Paul Atreides is extremely disciplined and confident in every thing he does. All of his actions and words are deliberate and precisely controlled, and it is this that makes him the super human being - Kwisatz Haderaca. The spice awakens his abilities but doesn't transform him into someone else. Total waste of money and time.\"\n",
        "texts = [text1, text2, text3, text4]"
      ],
      "metadata": {
        "id": "_Jqjcb2sH0zG"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: 상기 texts에 대해 예측해보세요.\n",
        "#Hint: tokenizer와 padding을 이용해야 합니다.\n",
        "tokens = tokenizer.texts_to_sequences(texts)\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "model.predict(tokens_pad)"
      ],
      "metadata": {
        "id": "VN2wWdHDJJNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24225a1-2ad7-4ab0-a34d-4fc5b2fbef2c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9988418 ],\n",
              "       [0.9893713 ],\n",
              "       [0.00709718],\n",
              "       [0.11264417]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THYTOZSet2Wd"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "임베딩을 통해 각 단어가 어떻게 변환되는지 살펴봅시다.\n",
        "\n",
        "The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n",
        "\n",
        "This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n",
        "\n",
        "Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values. Let us investigate if that has happened here.\n",
        "\n",
        "First we need to get the embedding-layer from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "jtyAVJbpt2Wd"
      },
      "outputs": [],
      "source": [
        "# 임베딩 층 불러오기\n",
        "layer_embedding = model.get_layer('layer_embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jANfbGKt2Wd"
      },
      "source": [
        "We can then get the weights used for the mapping done by the embedding-layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "uVO-XSXrt2We"
      },
      "outputs": [],
      "source": [
        "# 임베딩 층의 가중치 불러오기\n",
        "weights_embedding = layer_embedding.get_weights()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6qnaB0t2We"
      },
      "source": [
        "Note that the weights are actually just a matrix with the number of words in the vocabulary times the vector length for each embedding. That's because it is basically just a lookup-matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-YcE3C_kt2We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d86203f-29a7-4308-caea-57a467fe44f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "# 10,000개의 단어를 8차원으로 보내고 있습니다.\n",
        "weights_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwqGhFLVt2We"
      },
      "source": [
        "Let us get the integer-token for the word 'good', which is just an index into the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "M9xEoo2Ht2We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17140597-00a2-4c4c-c9b3-b93a903c0ad5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "# tokenizer에서 good의 위치\n",
        "token_good = tokenizer.word_index['good']\n",
        "token_good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1XR9Gut2We"
      },
      "source": [
        "Let us also get the integer-token for the word 'great'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "fFz5EWFPt2We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2bf59b-d824-42f1-bcf3-a2699e803960"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "# tokenizer에서 great의 위치\n",
        "token_great = tokenizer.word_index['great']\n",
        "token_great"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1itM760At2Wf"
      },
      "source": [
        "These integertokens may be far apart and will depend on the frequency of those words in the data-set.\n",
        "\n",
        "Now let us compare the vector-embeddings for the words 'good' and 'great'. Several of these values are similar, although some values are quite different. Note that these values will change every time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Ju_7MpFrt2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41cf3aee-2e3b-4487-c43e-fd3b02cb34be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.06917246,  0.07500466,  0.02024258, -0.06345233, -0.07336847,\n",
              "        0.04206818, -0.0595849 , -0.00975448], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "# good의 임베딩 결과\n",
        "weights_embedding[token_good]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "K4aw3__At2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70653fb-94e9-4d95-9f7d-49f6419eee8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.18121855,  0.14708585,  0.09529335, -0.15164873, -0.18549275,\n",
              "        0.09806824, -0.15447125, -0.09425583], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# great의 임베딩 결과\n",
        "weights_embedding[token_great]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrbHzmFlt2Wf"
      },
      "source": [
        "Similarly, we can compare the embeddings for the words 'bad' and 'horrible'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "WYOkp0DIt2Wf"
      },
      "outputs": [],
      "source": [
        "# tokenizer에서 bad와 horrible\n",
        "token_bad = tokenizer.word_index['bad']\n",
        "token_horrible = tokenizer.word_index['horrible']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "DhfbcEnit2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0f2f7e-c03b-4960-a28b-1196111dbfec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.07029106, -0.09501244, -0.12914671,  0.14653099,  0.14387043,\n",
              "       -0.1074779 ,  0.09129538,  0.06694379], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "# TODO: bad의 임베딩 결과\n",
        "weights_embedding[token_bad]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "8uqa8Krzt2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae6eb58-4d60-4aa8-ddd8-4c9ef9400ad7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.13663626, -0.17742288, -0.18578404,  0.15125124,  0.17309251,\n",
              "       -0.19713497,  0.19125336,  0.17514713], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "# TODO: horrible의 임베딩 결과\n",
        "weights_embedding[token_horrible]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩 결과를 보면 good과 great은 비슷한 곳에 위치하고 good과 bad는 반대되는 곳에 위치한 것을 볼 수 있습니다."
      ],
      "metadata": {
        "id": "0zOsin_OXnsU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gDSIg26t2Wf"
      },
      "source": [
        "### Sorted Words\n",
        "\n",
        "코사인 유사도로 임베딩 공간에서 비슷한 단어를 찾아봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "YyhM3RL0t2Wf"
      },
      "outputs": [],
      "source": [
        "# 주어진 단어와 비슷한 단어를 찾아주는 함수\n",
        "def print_sorted_words(word, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Print the words in the vocabulary sorted according to their\n",
        "    embedding-distance to the given word.\n",
        "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the token (i.e. integer ID) for the given word.\n",
        "    token = tokenizer.word_index[word]\n",
        "\n",
        "    # Get the embedding for the given word. Note that the\n",
        "    # embedding-weight-matrix is indexed by the word-tokens\n",
        "    # which are integer IDs.\n",
        "    embedding = weights_embedding[token]\n",
        "\n",
        "    # Calculate the distance between the embeddings for\n",
        "    # this word and all other words in the vocabulary.\n",
        "    distances = cdist(weights_embedding, [embedding],\n",
        "                      metric=metric).T[0]\n",
        "\n",
        "    # Get an index sorted according to the embedding-distances.\n",
        "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
        "    sorted_index = np.argsort(distances)\n",
        "\n",
        "    # Sort the embedding-distances.\n",
        "    sorted_distances = distances[sorted_index]\n",
        "\n",
        "    # Sort all the words in the vocabulary according to their\n",
        "    # embedding-distance. This is a bit excessive because we\n",
        "    # will only print the top and bottom words.\n",
        "    sorted_words = [inverse_map[token] for token in sorted_index\n",
        "                    if token != 0]\n",
        "\n",
        "    # Helper-function for printing words and embedding-distances.\n",
        "    def _print_words(words, distances):\n",
        "        for word, distance in zip(words, distances):\n",
        "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
        "\n",
        "    # Number of words to print from the top and bottom of the list.\n",
        "    k = 10\n",
        "\n",
        "    print(\"Distance from '{0}':\".format(word))\n",
        "\n",
        "    # Print the words with smallest embedding-distance.\n",
        "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
        "\n",
        "    print(\"...\")\n",
        "\n",
        "    # Print the words with highest embedding-distance.\n",
        "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewbOIavTt2Wg"
      },
      "source": [
        "We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "scrolled": true,
        "id": "CFU1vStzt2Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6014ca-40d1-4f71-c077-4ca06da3b9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'great':\n",
            "0.000 - great\n",
            "0.008 - symbolism\n",
            "0.008 - enjoyed\n",
            "0.009 - voight\n",
            "0.009 - extraordinary\n",
            "0.009 - lily\n",
            "0.009 - surrender\n",
            "0.010 - sissy\n",
            "0.010 - deemed\n",
            "0.011 - jackie\n",
            "...\n",
            "1.986 - disappointment\n",
            "1.987 - mundane\n",
            "1.988 - unfunny\n",
            "1.989 - chick\n",
            "1.990 - continually\n",
            "1.990 - fiasco\n",
            "1.991 - actor's\n",
            "1.991 - dvd's\n",
            "1.992 - fiennes\n",
            "1.993 - contrived\n"
          ]
        }
      ],
      "source": [
        "# great와 비슷한 단어 10개, 반대 단어 10개\n",
        "print_sorted_words('great', metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZi34Trt2Wg"
      },
      "source": [
        "Similarly, we can print the words that are near and far from the word 'worst' in terms of their vector-embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "scrolled": true,
        "id": "P3qmBJ95t2Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793f2475-2603-4bc2-d533-213ea13b0d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'worst':\n",
            "0.000 - worst\n",
            "0.003 - lindsay\n",
            "0.003 - mercifully\n",
            "0.004 - atrocious\n",
            "0.004 - fodder\n",
            "0.004 - jerky\n",
            "0.004 - quantum\n",
            "0.005 - unremarkable\n",
            "0.005 - embarrassed\n",
            "0.006 - ham\n",
            "...\n",
            "1.995 - excellent\n",
            "1.996 - demille\n",
            "1.996 - devotion\n",
            "1.996 - gruff\n",
            "1.996 - fascinating\n",
            "1.996 - owns\n",
            "1.996 - neglected\n",
            "1.996 - byrne\n",
            "1.997 - favorites\n",
            "1.997 - quintessential\n"
          ]
        }
      ],
      "source": [
        "# TODO: worst와 비슷한 단어 10개, 반대 단어 10개\n",
        "print_sorted_words('worst',metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdkL2Jubt2Wg"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "감성 분석에서 순환 신경망은 비교적 만족스러운 성능을 보이지만 사람과는 전혀 다른 방식으로 감정을 계산해냅니다. 거대 언어 모델에 비하여 상당히 적은 자원만을 사용했기 때문에 성능이 좋게 나오지 않았으니 참고하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnclb77Ct2Wg"
      },
      "source": [
        "## 연습문제 (수행할 필요 없습니다.)\n",
        "\n",
        "수행할 때마다 다른 결과를 얻을 수 있습니다.\n",
        "\n",
        "* 현재 Tokenizer가 가장 자주 쓰이는 10,000개의 단어를 처리하는데 5000개만 사용하면 성능이 어떻게 변할까요?\n",
        "* 임베딩을 8차원에서 수행하는데 200차원으로 늘리면 성능이 어떻게 변할까요?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTjDZ271t2Wg"
      },
      "source": [
        "## License (MIT)\n",
        "\n",
        "Copyright (c) 2022 by uramoon@kw.ac.kr<br>\n",
        "Copyright (c) 2018 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZdkL2Jubt2Wg",
        "wnclb77Ct2Wg",
        "NTjDZ271t2Wg"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}